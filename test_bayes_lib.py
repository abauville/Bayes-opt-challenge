import pytestimport bayes_libimport torchfrom botorch.test_functions.synthetic import Hartmannfrom gpytorch.likelihoods import GaussianLikelihoodfrom gpytorch.constraints import Intervalfrom botorch.acquisition.analytic import ExpectedImprovementdef gt_func(x):    """Ground truth function: negative hartmann 6    The Bayes opt library we use aims to maximize functions by default.    We use the negated function to effectively minimize it, i.e. argmin(f(x)) = argmax(-f(x))    """    hart = Hartmann()    return - hart.evaluate_true(x)def error_gap(current_best):    """Returns the absolute difference between the global minimum of the hartmann 6 function    and the current best value            Error gap := |min f(x*) -  current best|    """    hart = Hartmann()    return abs(current_best - (- hart.optimal_value))def get_model():    device = 'cpu'    train_x = torch.tensor(        [0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573],         dtype=torch.float64).reshape(-1,6).to(device)    train_y = gt_func(train_x).to(device)    likelihood = GaussianLikelihood(noise_constraint=Interval(0.0,1e-6)).to(device)    model = bayes_lib.ExactGPModel(train_x, train_y, likelihood).to(device)    return model, likelihoodclass TestModel():            def test_inference_return_type(self):        model, likelihood = get_model()        test_x = torch.rand(6*4, dtype=torch.float64).reshape(-1,6).to('cpu')        model.eval()        assert model(test_x) != None, "inference doesn't raise an error"                    def test_inference_return_value(self):        model, likelihood = get_model()        test_x = torch.tensor(            [0.20169, 0.150011, 0.476874, 0.275332, 0.311652, 0.6573],             dtype=torch.float64).reshape(-1,6).to('cpu')        test_x += torch.tensor(            [1e-14] * 6,             dtype=torch.float64).reshape(-1,6).to('cpu')                model.eval()        likelihood.eval()        assert error_gap(likelihood(model(test_x)).loc.item()) < 1e-4, (            "the error gap at a the location of a training_point should be close to zero")    def test_train_hyper_params(self):        model, likelihood = get_model()        params_before = list(model.parameters())[0].item()        bayes_lib.train_hyper_params(model, likelihood)        params_after = list(model.parameters())[0].item()        assert params_before != params_after , "train_hyper_params did not modify the model's parameters"                    def test_get_x_new(self):        model, likelihood = get_model()        EI = ExpectedImprovement(model, best_f=0.1, maximize=True)        assert bayes_lib.get_x_new(EI).shape == torch.Size([6]), "get_x_new must return a tensor of shape [6]"                    def test_run_experiment(self):        assert run_experiment(gt_func).shape == (1,), "run experiment must return a list of the best values"                                                        